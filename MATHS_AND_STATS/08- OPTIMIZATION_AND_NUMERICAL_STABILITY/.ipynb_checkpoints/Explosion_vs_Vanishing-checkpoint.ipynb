{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab21558f-d5f6-4db8-a925-4ff2bbcaa289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4eb1f8b-721d-4b62-b53b-f881aa608cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanishing Gradient after 50 layers: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 1. Simulating Vanishing Gradient\n",
    "# Many layers with small weights\n",
    "weights = torch.tensor([0.1])\n",
    "gradient = torch.tensor([1.0])\n",
    "\n",
    "for i in range(50):\n",
    "    gradient = gradient * weights\n",
    "\n",
    "print(f\"Vanishing Gradient after 50 layers: {gradient.item()}\") # Very close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6eb444e1-908b-440a-a2d2-c9e332eda134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Exploded at layer 127!\n"
     ]
    }
   ],
   "source": [
    "# 2. Simulating Exploding Gradient\n",
    "# Many layers with large weights\n",
    "weights_large = torch.tensor([2.0])\n",
    "gradient_large = torch.tensor([1.0])\n",
    "\n",
    "for i in range(150):\n",
    "    gradient_large = gradient_large * weights_large\n",
    "    if torch.isinf(gradient_large):\n",
    "        print(f\"Gradient Exploded at layer {i}!\")\n",
    "        break\n",
    "else:\n",
    "    print(\"Gradient not Exploded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65ad4ff4-419b-40b5-b7da-4c45f1fdf774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 1000.0 | Clipped: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Gradient clipping for gredient explosion\n",
    "# Simulating a very large gradient\n",
    "gradient = torch.tensor([1000.0])\n",
    "max_norm = 1.0 # Our safety limit\n",
    "\n",
    "# Logic: If gradient > max_norm, scale it down\n",
    "if gradient.norm() > max_norm:\n",
    "    clipped_gradient = (gradient / gradient.norm()) * max_norm\n",
    "\n",
    "# In PyTorch, we use this simple line:\n",
    "# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "print(f\"Original: {gradient.item()} | Clipped: {clipped_gradient.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc55b923-73ac-40a1-b743-429a00df8614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization for gredient explosion\n",
    "\n",
    "# In a Neural Network layer\n",
    "layer = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.BatchNorm1d(10), # Normalizing the outputs\n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41e32628-3a23-4004-b1d9-378af3c2ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight normalization for gredient explosion\n",
    "\n",
    "#Logic: Initializing weights based on layer size\n",
    "input_dim = 100\n",
    "output_dim = 50\n",
    "\n",
    "# He Initialization for ReLU layers\n",
    "std = torch.sqrt(torch.tensor(2.0 / input_dim))\n",
    "weights = torch.randn(input_dim, output_dim) * std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
